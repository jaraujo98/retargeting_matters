<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="description"
        content="Retargeting Matters: General Motion Retargeting for Humanoid Motion Tracking.">
  <meta name="keywords" content="Humanoid, Retargeting, Motion">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>Retargeting Matters: General Motion Retargeting for Humanoid Motion Tracking</title>

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
        rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
        href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">
  <link rel="stylesheet" href="./static/css/related_work.css">

  <link rel="apple-touch-icon" sizes="57x57"
    href="//www-media.stanford.edu/assets/favicon/apple-touch-icon-57x57.png" />
  <link rel="apple-touch-icon" sizes="60x60"
    href="//www-media.stanford.edu/assets/favicon/apple-touch-icon-60x60.png" />
  <link rel="apple-touch-icon" sizes="72x72"
    href="//www-media.stanford.edu/assets/favicon/apple-touch-icon-72x72.png" />
  <link rel="apple-touch-icon" sizes="76x76"
    href="//www-media.stanford.edu/assets/favicon/apple-touch-icon-76x76.png" />
  <link rel="apple-touch-icon" sizes="114x114"
    href="//www-media.stanford.edu/assets/favicon/apple-touch-icon-114x114.png" />
  <link rel="apple-touch-icon" sizes="120x120"
    href="//www-media.stanford.edu/assets/favicon/apple-touch-icon-120x120.png" />
  <link rel="apple-touch-icon" sizes="144x144"
    href="//www-media.stanford.edu/assets/favicon/apple-touch-icon-144x144.png" />
  <link rel="apple-touch-icon" sizes="152x152"
    href="//www-media.stanford.edu/assets/favicon/apple-touch-icon-152x152.png" />
  <link rel="apple-touch-icon" sizes="180x180"
    href="//www-media.stanford.edu/assets/favicon/apple-touch-icon-180x180.png" />

  <link rel="icon" type="image/png" href="//www-media.stanford.edu/assets/favicon/favicon-196x196.png"
    sizes="196x196" />
  <link rel="icon" type="image/png" href="//www-media.stanford.edu/assets/favicon/favicon-192x192.png"
    sizes="192x192" />
  <link rel="icon" type="image/png" href="//www-media.stanford.edu/assets/favicon/favicon-128.png" sizes="128x128" />
  <link rel="icon" type="image/png" href="//www-media.stanford.edu/assets/favicon/favicon-96x96.png" sizes="96x96" />
  <link rel="icon" type="image/png" href="//www-media.stanford.edu/assets/favicon/favicon-32x32.png" sizes="32x32" />
  <link rel="icon" type="image/png" href="//www-media.stanford.edu/assets/favicon/favicon-16x16.png" sizes="16x16" />

  <link rel="mask-icon" href="//www-media.stanford.edu/assets/favicon/safari-pinned-tab.svg" color="#ffffff">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>
</head>
<body>


<section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <h1 class="title is-1 publication-title">Retargeting Matters: General Motion Retargeting for Humanoid Motion Tracking</h1>
          <div class="is-size-5 publication-authors">
            <span class="author-block">
              <a href="https://jaraujo98.github.io/">João Pedro Araújo</a><sup>†1</sup>,</span>
            <span class="author-block">
              <a href="https://yanjieze.com/">Yanjie Ze</a><sup>†1</sup>,</span>
            <span class="author-block">
              <a href="https://pei-xu.github.io/">Pei Xu</a><sup>†1</sup>,
            </span>
            <span class="author-block">
              <a href="https://jiajunwu.com/">Jiajun Wu</a><sup>*1</sup>,
            </span>
            <span class="author-block">
              <a href="https://tml.stanford.edu/">C. Karen Liu</a><sup>*1</sup>,
            </span>
          </div>

          <div class="is-size-5 publication-authors">
            <span class="author-block"><sup>†</sup>Equal contribution,</span>
            <span class="author-block"><sup>*</sup>Equal advising,</span>
            <span class="author-block"><sup>1</sup>Stanford University</span>
          </div>

          <div class="column has-text-centered">
            <div class="publication-links">
              <!-- PDF Link. -->
              <span class="link-block">
                <a href="http://arxiv.org/abs/2510.02252"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fas fa-file-pdf"></i>
                  </span>
                  <span>Paper</span>
                </a>
              </span>
              <span class="link-block">
                <a href="http://arxiv.org/abs/2510.02252"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv</span>
                </a>
              </span>
              <!-- Video Link. -->
              <span class="link-block">
                <a href="https://youtu.be/kr3I_g1GLYw"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-youtube"></i>
                  </span>
                  <span>Video</span>
                </a>
              </span>
              <!-- Code Link. -->
              <span class="link-block">
                <a href="https://github.com/YanjieZe/GMR"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-github"></i>
                  </span>
                  <span>Code</span>
                  </a>
              </span>
            </div>

          </div>
        </div>
      </div>
    </div>
  </div>
</section>


<section class="section">
  <div class="container is-max-desktop">
    <!-- Abstract. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            Humanoid motion tracking policies are central to building teleoperation pipelines and hierarchical controllers, yet they face a fundamental challenge: the embodiment gap between humans and humanoid robots. Current approaches address this gap by retargeting human motion data to humanoid embodiments and then training reinforcement learning (RL) policies to imitate these reference trajectories. However, artifacts introduced during retargeting, such as foot sliding, self-penetration, and physically infeasible motion are often left in the reference trajectories for the RL policy to correct. While prior work has demonstrated motion tracking abilities, they often require extensive reward engineering and domain randomization to succeed.
          </p>
          <p>
            In this paper, we systematically evaluate how retargeting quality affects policy performance when excessive reward tuning is suppressed. To address issues that we identify with existing retargeting methods, we propose a new retargeting method, General Motion Retargeting (GMR). We evaluate GMR alongside two open-source retargeters, PHC and ProtoMotions, as well as with a high-quality closed-source dataset from Unitree. Using BeyondMimic for policy training, we isolate retargeting effects without reward tuning.
          </p>
          <p>
            Our experiments on a diverse subset of the LAFAN1 dataset reveal that while most motions can be tracked, artifacts in retargeted data significantly reduce policy robustness, particularly for dynamic or long sequences. GMR consistently outperforms existing open-source methods in both tracking performance and faithfulness to the source motion, achieving perceptual fidelity and policy success rates close to the closed-source baseline.
          </p>
        </div>
      </div>
    </div>
    <!--/ Abstract. -->

    <!-- Paper video. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Video</h2>
        <div class="publication-video">
          <iframe src="https://www.youtube.com/embed/kr3I_g1GLYw?si=pxZ56qzaTii1V3t7" 
                  frameborder="0" allow="autoplay; encrypted-media;" allowfullscreen></iframe>
        </div>
      </div>
    </div>
    <!--/ Paper video. -->
  </div>
</section>


<section class="section">
  <div class="container is-max-desktop">

    <div class="columns is-centered">

      <!-- Motivation. -->
      <div class="column">
        <div class="content">
          <h2 class="title is-3">Motivation</h2>
          <p>
            The standard approach for overcoming the embodiment gap from humans to humanoids is to use kinematic retargeting from the source human motion to the target humanoid embodiment. This practice overlooks glaring artifacts introduced by the retargeting process (such as foot sliding, ground penetration, and physically impossible motion due to self-penetration), instead forcing the RL policy to imitate physically infeasible motions while maintaining physical constraints. Prior work has shown that while training policies on retargeted data with severe artifacts in simulation is possible, transferring them to the real world demands extensive trial-and-error, reward shaping, and parameter tuning. Considering this practice, our hypothesis is that <i>with enough engineering in the reward function and domain randomization, the artifacts caused by retargeting can be mostly mitigated or removed</i>. However, without these engineering efforts, <i>the quality of retargeting results plays a significant role</i>.
          </p>
          <video autoplay controls muted loop playsinline height="100%">
            <source src="./static/videos/artifacts.mp4"
                    type="video/mp4">
          </video>
        </div>
      <!--/ Motivation. -->

      <!-- GMR. -->
        <div class="content">
          <h2 class="title is-3">GMR: General Motion Retargeting</h2>
          <p>
            To address the artifacts (deviation from the source motion, foot sliding, ground penetrations, and self-intersections) we find in previous retargeting methods, we propose a new retargeting method, General Motion Retargeting (GMR). The main difference from the prior methods is how it handles source motion scaling, which we found to be the cause for many of the artifacts. This is followed by a two-stage optimization to find the robot motion.
          </p>
          <div class="content has-text-centered">
            <img src="./static/images/gmr_pipeline.png"
                 class="interpolation-image"
                 alt="Interpolate start reference image."/>
          </div>
        </div>
      </div>
      <!--/ GMR. -->
    </div>

    <!-- Evaluation. -->
    <div class="columns is-centered">
      <div class="column is-full-width">
        <h2 class="title is-3">Evaluation</h2>

        <!-- Data. -->
        <h3 class="title is-4">Data</h3>
        <div class="content has-text-justified">
          <p>
            We select a diverse sample from the LAFAN1 dataset, ranging from simple motions like walking and turning to dynamic and complex motions such as martial arts, kicks, and dancing. Our final dataset consists of 21 sequences with lengths ranging from 5 seconds to 2 minutes.
          </p>
          <video autoplay controls muted loop playsinline height="100%">
            <source src="./static/videos/training_data.mp4"
                    type="video/mp4">
          </video>
        </div>
        <br/>
        <!--/ Data. -->

        <!-- Method. -->
        <h3 class="title is-4">Method</h3>
        <div class="content has-text-justified">
          <p>
            We retarget each of the clips in the training data using the different methods and train single-clip motion imitation policies. We evaluate each policy in the training simulator with only observation noise (<b>sim</b>), domain randomization (<b>sim-dr</b>), and in a setup mimicking real-world deployment conditions (<b>sim2sim</b>).
          </p>
        </div>
        <br/>
        <!--/ Method. -->

        <!-- Results. -->
        <h3 class="title is-4">Results</h3>
        <div class="content has-text-justified">
          <div class="content has-text-centered">
            <img src="./static/images/sr_results_table.png"
                 class="interpolation-image"
                 alt="Interpolate start reference image."/>
          </div>
          <p>
            Our experiments reveal that most motions can be successfully tracked regardless of retargeting method. For the long-horizon or dynamic motions, GMR's success rate is close to Unitree retargeted dataset and outperforms other open-source methods. However, all three methods (including GMR) produce artifacts that impact the policy robustness:
          </p>

          <ul>
            <li>PHC retargets can exhibit severe ground penetration, which can make it difficult for the policy to learn to track a given motion.</li>
            <li>The ProtoMotions retarget for the "Run (stop & go)" motion has serious self-collisions.</li>
            <li>The GMR retarget for the "Dance 5" motion has sudden jumps in the joint values.</li>
          </ul>
          
          <p>These artifacts should be avoided in retargeted motion to ensure the best tracking results.</p>
        </div>
        <!--/ Results. -->

        <!-- Method. -->
        <h3 class="title is-4">User Study</h3>
        <div class="content has-text-justified">
          <p>
            In our experiments we find that the results of retargeting can deviate noticeably from the reference motion. To quantify this, we conduct a user study to measure the perceived faithfulness of the retargeted motion to the source human motion.
          </p>
          <div class="content has-text-centered">
            <img src="./static/images/user_study_results.png"
                 class="interpolation-image"
                 alt="Interpolate start reference image."/>
          </div>
          <p>Users consider the GMR retargets to be more faithful than the ones generated by the other methods, and very close to the Unitree retargets.</p>
        </div>
        <br/>
        <!--/ Method. -->

      </div>
    </div>
    <!--/ Animation. -->


    <!-- Related Work. -->
    <div class="columns is-centered">
      <div class="column is-full-width">
        <h2 class="title is-3">Related Work</h2>

        <div class="related-paper">
            <div class="related-paper-video">
                <video src="https://visualmimic.github.io/videos/twist.mp4" autoplay muted loop playsinline></video>
            </div>
            <div class="related-paper-content">
                <div class="related-paper-title">
                    <a href="https://yanjieze.com/TWIST/" target="_blank">TWIST: Teleoperated Whole-Body Imitation System</a>
                </div>
                <div class="related-paper-authors">Yanjie Ze<sup>*</sup>, Zixuan Chen<sup>*</sup>, João Pedro Araújo<sup>*</sup>, Zi-ang Cao, Xue Bin Peng, Jiajun Wu<sup>†</sup>, C. Karen Liu<sup>†</sup></div>
                <div class="related-paper-authors"><b>CoRL 2025</b></div>
                <div class="related-paper-links">
                    <a href="https://arxiv.org/abs/2505.02833" target="_blank">arXiv</a> /
                    <a href="https://yanjieze.com/TWIST/" target="_blank">Project Page</a> /
                    <a href="https://github.com/YanjieZe/TWIST" target="_blank">Code</a>
                </div>
            </div>
        </div>
        <div class="related-paper">
            <div class="related-paper-video">
                <video src="https://visualmimic.github.io/videos/real/push_box.mp4" autoplay muted loop playsinline></video>
            </div>
            <div class="related-paper-content">
                <div class="related-paper-title">
                    <a href="https://visualmimic.github.io/" target="_blank">VisualMimic: Visual Humanoid Loco-Manipulation via Motion Tracking and Generation</a>
                </div>
                <div class="related-paper-authors">Shaofeng Yin<sup>*</sup>, Yanjie Ze<sup>*</sup>, Hong-Xing Yu, C. Karen Liu<sup>†</sup>, Jiajun Wu<sup>†</sup></div>
                <div class="related-paper-links">
                    <a href="https://arxiv.org/abs/2509.20322" target="_blank">arXiv</a> /
                    <a href="https://visualmimic.github.io/" target="_blank">Project Page</a>
                </div>
            </div>
        </div>
      </div>
    </div>
    <!--/ Related Work. -->

  </div>
</section>


<section class="section" id="BibTeX">
  <div class="container is-max-desktop content">
    <h2 class="title">BibTeX</h2>
    <pre><code>
@article{araujo2025gmr,
  title={Retargeting Matters: General Motion Retargeting for Humanoid Motion Tracking},
  author= {Joao Pedro Araujo and Yanjie Ze and Pei Xu and Jiajun Wu and C. Karen Liu},
  year= {2025},
  journal= {arXiv preprint arXiv:2510.02252}
}
    </code></pre>
  </div>
</section>


<footer class="footer">
  <div class="container">
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">
          <p>Website template modified from <a href="https://nerfies.github.io/">NeRFies</a>.</p>
          <p>
            This website is licensed under a <a rel="license"
                                                href="http://creativecommons.org/licenses/by-sa/4.0/">Creative
            Commons Attribution-ShareAlike 4.0 International License</a>.
          </p>
          <p>
            This means you are free to borrow the <a
              href="https://github.com/nerfies/nerfies.github.io">source code</a> of this website,
            we just ask that you link back to this page in the footer.
            Please remember to remove the analytics code included in the header of the website which
            you do not want on your website.
          </p>
        </div>
      </div>
    </div>
  </div>
</footer>

</body>
</html>
